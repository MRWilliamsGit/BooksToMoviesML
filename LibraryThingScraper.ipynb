{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library thing scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0cfda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options \n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc36e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the function that searches for the movie title and date\n",
    "#it returns the title it finds (in case it is different than the search term) and a link to the first result\n",
    "def MovieFirst(baseurl, title):\n",
    "    \n",
    "    #initiate driver\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome('chromedriver', options=options)\n",
    "\n",
    "    #wait for page to load item, then get contents\n",
    "    try:\n",
    "        driver.get(baseurl.format(title))\n",
    "        timeout = 5\n",
    "        WebDriverWait(driver, timeout).until(ec.presence_of_element_located((By.CLASS_NAME, 'item')))\n",
    "        stuff = driver.page_source\n",
    "        driver.quit()\n",
    "\n",
    "        #get the first item title and link\n",
    "        soup = BeautifulSoup(stuff, features=\"html.parser\")\n",
    "        please = soup.find_all(attrs={'id':'ajaxcontent'})\n",
    "        please = soup.find_all(attrs={'class':'item'})\n",
    "        please = please[0].find('a')\n",
    "        title = please.string\n",
    "        link1 = please['href']\n",
    "        \n",
    "    except TimeoutException:\n",
    "        link1 = None\n",
    "\n",
    "    return title, link1\n",
    "\n",
    "#this is the function that receives a link (supposedly a movie pate) \n",
    "#it returns any 'adaptation of' info: book title and link \n",
    "def BookSecond(link1):\n",
    "    \n",
    "    #initiate driver\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    driver2 = webdriver.Chrome('chromedriver', options=options)\n",
    "    \n",
    "    #go to the link and pull the book title and link\n",
    "    try:\n",
    "        driver2.get('https://www.librarything.com'+link1)\n",
    "        timeout = 5\n",
    "        WebDriverWait(driver2, timeout).until(ec.presence_of_element_located((By.ID, 'relationships_container')))\n",
    "        stuff = driver2.page_source\n",
    "        driver2.quit()\n",
    "\n",
    "        #get the book title and link\n",
    "        soup = BeautifulSoup(stuff, features=\"html.parser\")\n",
    "        yes = soup.find(attrs={'id':'relationships_container'})\n",
    "        hm = soup.find(text=\"Is an adaptation of\")\n",
    "        if type(hm) == type(None):\n",
    "            book = None\n",
    "            link2 = None\n",
    "        else:\n",
    "            yes = soup.find(text=\"Is an adaptation of\").find_next()\n",
    "            yes = yes.find(attrs={'class':'popup_registered'})\n",
    "            book = yes.string\n",
    "            link2 = yes['href']\n",
    "\n",
    "    except TimeoutException:\n",
    "        book = None\n",
    "        link2 = None\n",
    "\n",
    "    return book, link2\n",
    "\n",
    "#this function receives a movie title and year\n",
    "#it uses the \"MovieFirst\" and 'BookSecond' functions\n",
    "#it returns the found movie title, the link to the movie entry, the book title, and the link to the book entry\n",
    "def ScrapeSearch(title, moviedate):\n",
    "\n",
    "        #url puzzle pieces\n",
    "        baseurl = 'https://www.librarything.com/search.php?search={}&searchtype=newwork_titles&searchtype=newwork_titles&sortchoice=0'\n",
    "        search = title + ' ['+ str(moviedate)+ ' film]'    \n",
    "   \n",
    "        #search by movie and date\n",
    "        newtitle, link1 = MovieFirst(baseurl, search)\n",
    "        #if no results, search by movie title only\n",
    "        if link1 == None:\n",
    "            newtitle, link1 = MovieFirst(baseurl, title)\n",
    "            #if still nothing, oh well\n",
    "            if link1 == None:\n",
    "                book = None\n",
    "                link2 =None\n",
    "            #otherwise, check that one for adapatation info  \n",
    "            else:\n",
    "                book, link2 = BookSecond(link1)\n",
    "        #if movie results found, go into movie entry and get the book title and link\n",
    "        else:\n",
    "            book, link2 = BookSecond(link1)\n",
    "            #if no book adaptation info, search again by just the movie title\n",
    "            #assume first result without 'film]' is the book\n",
    "            if link2 == None:\n",
    "                book, link2 = MovieFirst(baseurl, title)\n",
    "            \n",
    "        #return them\n",
    "        return [newtitle, link1, book, link2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88b80602",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CleanMovieData.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-39a9770e0351>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#pull in the IMDb list (1373 lines)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmoviesdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CleanMovieData.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CleanMovieData.csv'"
     ]
    }
   ],
   "source": [
    "#pull in the IMDb list (1373 lines)\n",
    "moviesdb = pd.read_csv('CleanMovieData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7170a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loop through the movie list and make a table\n",
    "ohgood = pd.DataFrame(columns = ['SearchTerm', 'Link1', 'AdaptationOf', 'Link2'])\n",
    "\n",
    "moviesdb = moviesdb.iloc[49:]\n",
    "\n",
    "for hm in moviesdb.index:\n",
    "    hello = ScrapeSearch(moviesdb.Title[hm], moviesdb.Date[hm])\n",
    "    #print(hello)\n",
    "    ohgood = ohgood.append(pd.Series(hello, index = ohgood.columns), ignore_index=True)\n",
    "\n",
    "    \n",
    "#NOTE: if the Search term does NOT have the [] and there is no Adaptation link, can assume the first link is the book\n",
    "#NOTE: if the Search term DOES have the [], and there is no Adaptation Link, \n",
    "#maybe the first result without 'film]' is the associate book \n",
    "\n",
    "#none of these assumptions are coded yet we are in a hurry - should compare key words to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "1324eed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   SearchTerm    50 non-null     object\n",
      " 1   Link1         50 non-null     object\n",
      " 2   AdaptationOf  38 non-null     object\n",
      " 3   Link2         38 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ohgood.info())\n",
    "\n",
    "#pulls the tag cloud - not sure where the check comes from \n",
    "#content = requests.get(\"http://www.librarything.com/ajax_work_makeworkCloud.php?work=3203347&check=2801929225\").text\n",
    "#soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "7815191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ohgood.to_csv('LTlinks1.csv', index = False)\n",
    "#first pull was 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e3bf38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
