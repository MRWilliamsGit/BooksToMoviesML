{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e057b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maria Williams  - Nov, 2021\n",
    "#This script scrapes IMDb for film information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935a1b3",
   "metadata": {},
   "source": [
    "# First Step: \n",
    "Scrape list of movies tagged as 'based on novel' and released from years 1970 to 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b14fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09aba836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Pass:\n",
    "#1) feature films, 2) tagged 'based on novel', 3) release from years 2000 to 2021: 3,516 titles - 71 pages\n",
    "#baseurl = 'https://www.imdb.com/search/keyword/?keywords=based-on-novel&ref_=kw_nxt&mode=detail&page={}&title_type=movie&release_date=2000%2C2021&sort=year,desc'\n",
    "\n",
    "#Second Pass:\n",
    "#1) feature films, 2) tagged 'based on novel', 3) release from years 1970 to 2000: 6,262 titles - 126 pages\n",
    "baseurl = 'https://www.imdb.com/search/keyword/?keywords=based-on-novel&ref_=kw_ref_yr&mode=detail&page={}&title_type=movie&release_date=1970%2C2000&sort=year,desc'\n",
    "\n",
    "#initialize dataframe\n",
    "popcorn = pd.DataFrame(columns = ['Link','Title', 'Release_Date', 'ViewRating', 'Runtime', 'Genres', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24b5ffd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#little function to extract text from bs4 or return blank string\n",
    "def makestring(bs4):\n",
    "    if bs4 is None:\n",
    "        return ' '\n",
    "    else:\n",
    "        return bs4.string\n",
    "    \n",
    "#function to scrape list page\n",
    "def GetPage(baseurl,num):\n",
    "    \n",
    "    pop = pd.DataFrame(columns = ['Link','Title', 'Release_Date', 'ViewRating', 'Runtime', 'Genres', 'Description'])\n",
    "    \n",
    "    page = requests.get(baseurl.format(num))\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    main = soup.find_all(attrs={'class':'lister-item mode-detail'})\n",
    "    #print(len(main))\n",
    "\n",
    "    for p in range(len(main)):\n",
    "        #get title\n",
    "        link = main[p].a['href']\n",
    "        title = main[p].h3.a.string\n",
    "        #get description\n",
    "        desc = main[p].find_all('p')\n",
    "        desc = desc[1].string\n",
    "        #get a bunch of other things that may or may not be there  \n",
    "        year = makestring(main[p].find(attrs={'class':'lister-item-year text-muted unbold'}))\n",
    "        rating = makestring(main[p].find(attrs={'class':'certificate'}))\n",
    "        length = makestring(main[p].find(attrs={'class':'runtime'}))\n",
    "        genre = makestring(main[p].find(attrs={'class':'genre'}))\n",
    "        gotit = [link,title,year,rating,length,genre,desc]\n",
    "        pop.loc[len(pop)] = gotit\n",
    "\n",
    "    #print(pop.info())\n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4493c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100 entries, 0 to 49\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Link          100 non-null    object\n",
      " 1   Title         100 non-null    object\n",
      " 2   Release_Date  100 non-null    object\n",
      " 3   ViewRating    100 non-null    object\n",
      " 4   Runtime       100 non-null    object\n",
      " 5   Genres        100 non-null    object\n",
      " 6   Description   79 non-null     object\n",
      "dtypes: object(7)\n",
      "memory usage: 6.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#each page has 50 entries\n",
    "##First pass: loop to get a 71 pages\n",
    "##Second pass: loop to get 126 pages\n",
    "\n",
    "#hard coded here as 2 pages\n",
    "for page in range(2):\n",
    "    popper = GetPage(baseurl,page+1)\n",
    "    popcorn = popcorn.append(popper)\n",
    "\n",
    "popcorn['Genres'] = popcorn['Genres'].str.strip()\n",
    "popcorn['Description'] = popcorn['Description'].str.strip()\n",
    "\n",
    "print(popcorn.info())\n",
    "#print(popcorn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c13c1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export dataframe\n",
    "#popcorn.to_csv('FilmList2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05db22",
   "metadata": {},
   "source": [
    "# Second Step: \n",
    "Take list and scrape full IMDB page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37334961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick function to get text from between brackets\n",
    "#turns out I could have just used .string ><\n",
    "def findtext(w):\n",
    "    w = str(w)\n",
    "    w = w[w.find('>')+1:w.find('<', w.find('>'), len(w))]\n",
    "    return w\n",
    "\n",
    "#quick function to get money - does not convert to string at this time\n",
    "def findmoney(w):\n",
    "    w = str(w)\n",
    "    w = w[w.find('$')+1:w.find('<', w.find('$'), len(w))]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d27ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "###THIS IS THE FULL SCRAPE OF A PAGE###\n",
    "\n",
    "def Get_Movie(url):\n",
    "    # Request the page and use BeautifulSoup to extract the contents\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    #get title\n",
    "    title = soup.find(attrs={'data-testid':'hero-title-block__title'})\n",
    "    title = findtext(title)\n",
    "\n",
    "    #get release date and rating (PG, PG13, etc)\n",
    "    DR = soup.find_all(attrs={'class':'TitleBlockMetaData__ListItemText-sc-12ein40-2 jedhex'})\n",
    "    if DR is not None:\n",
    "        if len(DR)==2:\n",
    "            date = findtext(DR[0])\n",
    "            rating = findtext(DR[1])\n",
    "        else:\n",
    "            date = findtext(DR)\n",
    "            rating = None\n",
    "    else:\n",
    "        date = None\n",
    "        rating = None\n",
    "\n",
    "    #get a list of genres\n",
    "    gen = soup.find(attrs={'data-testid':'storyline-genres'}) \n",
    "    if gen is not None:\n",
    "        gen = gen.find_all('a')\n",
    "        genres =''\n",
    "        for t in range(len(gen)):\n",
    "            genres = genres + findtext(gen[t]) + ' '\n",
    "    else:\n",
    "        genres = None\n",
    "\n",
    "    #get box office info\n",
    "    #budget often has '(estimated)' in it\n",
    "    box = soup.find(attrs={'data-testid':'title-boxoffice-section'})\n",
    "    if box is not None:\n",
    "        budget = box.find(attrs={'data-testid':'title-boxoffice-budget'}) \n",
    "        budget = str(budget)\n",
    "        budget = findmoney(budget)\n",
    "        dom = box.find(attrs={'data-testid':'title-boxoffice-grossdomestic'})\n",
    "        dom = findmoney(dom)\n",
    "        ww = soup.find(attrs={'data-testid':'title-boxoffice-cumulativeworldwidegross'}) \n",
    "        ww = findmoney(ww)\n",
    "    else:\n",
    "        budget = None\n",
    "        dom = None\n",
    "        ww = None\n",
    "\n",
    "    \n",
    "    story = soup.find(attrs={'data-testid':'Storyline'}) \n",
    "    if story is not None:\n",
    "        #get the description\n",
    "        desc = story.find(attrs={'class':'ipc-html-content ipc-html-content--base'})\n",
    "        if desc is not None:\n",
    "            desc = desc.div\n",
    "            desc = findtext(desc)\n",
    "        else:\n",
    "            desc = ''\n",
    "        #get top tags\n",
    "        #only shows top few tags, usually list tag is to full list which can be in hundreds\n",
    "        kw = story.find(attrs={'data-testid':'storyline-plot-keywords'}) \n",
    "        if kw is not None:\n",
    "            kw = kw.find_all('span')\n",
    "            keywords = ''\n",
    "            for t in range(len(kw)):\n",
    "                #keywords.append(findtext(kw[t]))\n",
    "                keywords = keywords + findtext(kw[t]) + ' '\n",
    "        else:\n",
    "            keywords = None\n",
    "    else:\n",
    "        desc = None\n",
    "        keywords = None\n",
    "\n",
    "    #get runtime in minutes\n",
    "    rt = soup.find(attrs={'data-testid':'title-techspec_runtime'}) \n",
    "    if rt is not None:\n",
    "        rt = rt.find('div')\n",
    "        hrs = findtext(rt)\n",
    "        rt = str(rt)\n",
    "        mins = rt[::-1]\n",
    "        mins = mins[7:]\n",
    "        mins = mins[mins.find('<')+1:mins.find('>', mins.find('<'), len(mins))]\n",
    "        if hrs==' ':\n",
    "            runtime = int(mins)\n",
    "        elif mins==' ':\n",
    "            runtime = int(hrs)*60\n",
    "        else:\n",
    "            runtime = int(hrs)*60 + int(mins)\n",
    "    else:\n",
    "        runtime = None\n",
    "\n",
    "    #put all that info in a list and return it \n",
    "    obs = [title,date,rating, genres, budget,dom,ww,desc,keywords, runtime]\n",
    "    \n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "584d4736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100 entries, 0 to 99\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Title           100 non-null    object\n",
      " 1   Release_Date    100 non-null    object\n",
      " 2   ViewRating      65 non-null     object\n",
      " 3   Genres          100 non-null    object\n",
      " 4   Budget          69 non-null     object\n",
      " 5   DomesticGross   69 non-null     object\n",
      " 6   WorldwideGross  69 non-null     object\n",
      " 7   Description     100 non-null    object\n",
      " 8   Keywords        100 non-null    object\n",
      " 9   Runtime         100 non-null    object\n",
      "dtypes: object(10)\n",
      "memory usage: 8.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#ok now pull in our list of movies\n",
    "#movies = pd.read_csv(r'C:\\Users\\maria\\OneDrive\\Documents\\AIPI510\\BooksToMoviesML\\FilmList.csv')   <--- original 2000-2021 list\n",
    "#movies = pd.read_csv(r'C:\\Users\\maria\\OneDrive\\Documents\\AIPI510\\BooksToMoviesML\\FilmList2.csv')  <--- original 1970-2000 list\n",
    "#movies = movies['Link']\n",
    "\n",
    "#to use full script instead of files\n",
    "movies = popcorn['Link']\n",
    "\n",
    "#instantiate dataframe\n",
    "data = pd.DataFrame(columns = ['Title', 'Release_Date', 'ViewRating', 'Genres', 'Budget', 'DomesticGross', \n",
    "                              'WorldwideGross', 'Description', 'Keywords', 'Runtime'])\n",
    "\n",
    "#scrape provided pages\n",
    "#this counter is because i had to run it in batches\n",
    "#movies = movies[4610:]\n",
    "\n",
    "for this in movies:\n",
    "    #print(this)\n",
    "    obs = Get_Movie('https://www.imdb.com{}'.format(this))\n",
    "    data.loc[len(data)] = obs\n",
    "    \n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('New1652.csv',index=False)\n",
    "#2106 first pull, 2505 next pull, 1652 last pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca6283ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'New2106.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4a0054794406>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#this cell is to combine different batch files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'New2106.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtwo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'New2505.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mthree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'New1652.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'New2106.csv'"
     ]
    }
   ],
   "source": [
    "#this cell is to combine different batch files\n",
    "one = pd.read_csv('New2106.csv')\n",
    "two = pd.read_csv('New2505.csv')\n",
    "three = pd.read_csv('New1652.csv')\n",
    "\n",
    "together = pd.concat([one,two,three])\n",
    "together.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "print(together.info())\n",
    "#together.to_csv('IMDbScrape2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4c302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
