{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "388d5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library thing scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0cfda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options \n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc36e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the function that searches for the movie title and date\n",
    "#it returns the title it finds (in case it is different than the search term) and a link to the first result\n",
    "def MovieFirst(baseurl, title):\n",
    "    \n",
    "    #initiate driver\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome('chromedriver', options=options)\n",
    "\n",
    "    #wait for page to load item, then get contents\n",
    "    try:\n",
    "        driver.get(baseurl.format(title))\n",
    "        timeout = 5\n",
    "        WebDriverWait(driver, timeout).until(ec.presence_of_element_located((By.CLASS_NAME, 'item')))\n",
    "        stuff = driver.page_source\n",
    "        driver.quit()\n",
    "\n",
    "        #get the first item title and link\n",
    "        soup = BeautifulSoup(stuff, features=\"html.parser\")\n",
    "        please = soup.find_all(attrs={'id':'ajaxcontent'})\n",
    "        please = soup.find_all(attrs={'class':'item'})\n",
    "        please = please[0].find('a')\n",
    "        title = please.string\n",
    "        link1 = please['href']\n",
    "        \n",
    "    except TimeoutException:\n",
    "        link1 = None\n",
    "\n",
    "    return title, link1\n",
    "\n",
    "#this is the function that receives a link (supposedly a movie pate) \n",
    "#it returns any 'adaptation of' info: book title and link \n",
    "def BookSecond(link1):\n",
    "    \n",
    "    #initiate driver\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    driver2 = webdriver.Chrome('chromedriver', options=options)\n",
    "    \n",
    "    #go to the link and pull the book title and link\n",
    "    try:\n",
    "        driver2.get('https://www.librarything.com'+link1)\n",
    "        timeout = 5\n",
    "        WebDriverWait(driver2, timeout).until(ec.presence_of_element_located((By.ID, 'relationships_container')))\n",
    "        stuff = driver2.page_source\n",
    "        driver2.quit()\n",
    "\n",
    "        #get the book title and link\n",
    "        soup = BeautifulSoup(stuff, features=\"html.parser\")\n",
    "        yes = soup.find(attrs={'id':'relationships_container'})\n",
    "        hm = soup.find(text=\"Is an adaptation of\")\n",
    "        if type(hm) == type(None):\n",
    "            book = None\n",
    "            link2 = None\n",
    "        else:\n",
    "            yes = soup.find(text=\"Is an adaptation of\").find_next()\n",
    "            yes = yes.find(attrs={'class':'popup_registered'})\n",
    "            book = yes.string\n",
    "            link2 = yes['href']\n",
    "\n",
    "    except TimeoutException:\n",
    "        book = None\n",
    "        link2 = None\n",
    "\n",
    "    return book, link2\n",
    "\n",
    "#this function receives a movie title and year\n",
    "#it uses the \"MovieFirst\" and 'BookSecond' functions from above\n",
    "#it returns the found movie title, the link to the movie entry, the book title, and the link to the book entry\n",
    "def ScrapeSearch(title, moviedate):\n",
    "\n",
    "        #url puzzle pieces\n",
    "        baseurl = 'https://www.librarything.com/search.php?search={}&searchtype=newwork_titles&searchtype=newwork_titles&sortchoice=0'\n",
    "        search = title + ' ['+ str(moviedate)+ ' film]'    \n",
    "   \n",
    "        #search by movie and date\n",
    "        newtitle, link1 = MovieFirst(baseurl, search)\n",
    "        #if no results, search by movie title only\n",
    "        if link1 == None:\n",
    "            newtitle, link1 = MovieFirst(baseurl, title)\n",
    "            #if still nothing, oh well\n",
    "            if link1 == None:\n",
    "                book = None\n",
    "                link2 =None\n",
    "            #otherwise, check that one for adapatation info  \n",
    "            else:\n",
    "                book, link2 = BookSecond(link1)\n",
    "        #if movie results found, go into movie entry and get the book title and link\n",
    "        else:\n",
    "            book, link2 = BookSecond(link1)\n",
    "            #if no book adaptation info, search again by just the movie title\n",
    "            #assume first result without 'film]' is the book\n",
    "            if link2 == None:\n",
    "                book, link2 = MovieFirst(baseurl, title)\n",
    "            \n",
    "        #return them\n",
    "        return [newtitle, link1, book, link2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88b80602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in the IMDb list (1373 lines)\n",
    "moviesdb = pd.read_csv('CleanMovieData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7170a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loop through the movie list and make a table\n",
    "ohgood = pd.DataFrame(columns = ['SearchTerm', 'Link1', 'AdaptationOf', 'Link2'])\n",
    "\n",
    "moviesdb = moviesdb.iloc[865:]\n",
    "\n",
    "for hm in moviesdb.index:\n",
    "    hello = ScrapeSearch(moviesdb.Title[hm], moviesdb.Date[hm])\n",
    "    #print(hello)\n",
    "    ohgood = ohgood.append(pd.Series(hello, index = ohgood.columns), ignore_index=True)\n",
    "\n",
    "    \n",
    "#NOTE: if the Search term does NOT have the [] and there is no Adaptation link, can assume the first link is the book\n",
    "#NOTE: if the Search term DOES have the [], and there is no Adaptation Link, \n",
    "#maybe the first result without 'film]' is the associate book \n",
    "\n",
    "#none of these assumptions are coded yet we are in a hurry - should compare key words to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1324eed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 509 entries, 0 to 508\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   SearchTerm    509 non-null    object\n",
      " 1   Link1         506 non-null    object\n",
      " 2   AdaptationOf  454 non-null    object\n",
      " 3   Link2         453 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 16.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ohgood.info())\n",
    "\n",
    "#pulls the tag cloud - not sure where the check comes from \n",
    "#content = requests.get(\"http://www.librarything.com/ajax_work_makeworkCloud.php?work=3203347&check=2801929225\").text\n",
    "#soup = BeautifulSoup(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7815191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohgood.to_csv('LTlinks5.csv', index = False)\n",
    "#first pull was 50\n",
    "#second pull 385\n",
    "#third pull 352\n",
    "#fourth pull started 12:18, ended 12.38 82\n",
    "#fifth pull started 12:39 ended at 2:50 509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8e3bf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1340 entries, 0 to 1339\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   SearchTerm    1340 non-null   object\n",
      " 1   Link1         1321 non-null   object\n",
      " 2   AdaptationOf  1126 non-null   object\n",
      " 3   Link2         1125 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 42.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#this cell is just to slap everything together since I had trouble running the full scrape\n",
    "\n",
    "one = pd.read_csv('LTlinks1.csv')\n",
    "two = pd.read_csv('LTlinks2.csv')\n",
    "three = pd.read_csv('LTlinks3.csv')\n",
    "four = pd.read_csv('LTlinks4.csv')\n",
    "five = pd.read_csv('LTlinks5.csv')\n",
    "\n",
    "together = pd.concat([one,two,three,four,five], ignore_index=True)\n",
    "together.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "print(together.info())\n",
    "\n",
    "together.to_csv('LTlinksFull.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9812adf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
