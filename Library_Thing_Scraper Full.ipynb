{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a06b1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maria Williams  - Nov, 2021\n",
    "#Library thing scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7c36a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\maria\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from selenium) (0.19.0)\n",
      "Requirement already satisfied: urllib3[secure]~=1.26 in c:\\users\\maria\\appdata\\roaming\\python\\python38\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\maria\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\maria\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (20.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.5)\n",
      "Requirement already satisfied: idna in c:\\users\\maria\\appdata\\roaming\\python\\python38\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\maria\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\maria\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.0.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\maria\\appdata\\roaming\\python\\python38\\site-packages (from urllib3[secure]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (20.0.1)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from urllib3[secure]~=1.26->selenium) (3.4.7)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\maria\\appdata\\roaming\\python\\python38\\site-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\maria\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.12.0)\n",
      "Requirement already satisfied: chromedriver-py in c:\\users\\maria\\anaconda3\\lib\\site-packages (95.0.4638.17)\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options \n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "\n",
    "#this script uses selenium and chromedriver to crawl the page\n",
    "!pip install selenium\n",
    "!pip install chromedriver-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776b277",
   "metadata": {},
   "source": [
    "# First Step:\n",
    "Get links to the book pages that correspond to a list of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ef6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the function that searches for the movie title and release date in the standard format used by Library Thing\n",
    "#it returns the title it finds (in case it is different than the search term) and a link to the first result\n",
    "def MovieFirst(baseurl, title):\n",
    "    \n",
    "    #initiate driver\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome('chromedriver', options=options)\n",
    "\n",
    "    #wait for page to load item, then get contents\n",
    "    try:\n",
    "        driver.get(baseurl.format(title))\n",
    "        timeout = 5\n",
    "        WebDriverWait(driver, timeout).until(ec.presence_of_element_located((By.CLASS_NAME, 'item')))\n",
    "        stuff = driver.page_source\n",
    "        driver.quit()\n",
    "\n",
    "        #get the first item title and link\n",
    "        soup = BeautifulSoup(stuff, features=\"html.parser\")\n",
    "        please = soup.find_all(attrs={'id':'ajaxcontent'})\n",
    "        please = soup.find_all(attrs={'class':'item'})\n",
    "        please = please[0].find('a')\n",
    "        title = please.string\n",
    "        link1 = please['href']\n",
    "        \n",
    "    except TimeoutException:\n",
    "        link1 = None\n",
    "\n",
    "    return title, link1\n",
    "\n",
    "#this is the function that receives a link (supposedly a movie page) \n",
    "#it returns any 'adaptation of' info: book title and link \n",
    "def BookSecond(link1):\n",
    "    \n",
    "    #initiate driver\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    driver2 = webdriver.Chrome('chromedriver', options=options)\n",
    "    \n",
    "    try:\n",
    "        driver2.get('https://www.librarything.com'+link1)\n",
    "        timeout = 5\n",
    "        WebDriverWait(driver2, timeout).until(ec.presence_of_element_located((By.ID, 'relationships_container')))\n",
    "        stuff = driver2.page_source\n",
    "        driver2.quit()\n",
    "\n",
    "        #get the book title and link\n",
    "        soup = BeautifulSoup(stuff, features=\"html.parser\")\n",
    "        yes = soup.find(attrs={'id':'relationships_container'})\n",
    "        hm = soup.find(text=\"Is an adaptation of\")\n",
    "        if type(hm) == type(None):\n",
    "            book = None\n",
    "            link2 = None\n",
    "        else:\n",
    "            yes = soup.find(text=\"Is an adaptation of\").find_next()\n",
    "            yes = yes.find(attrs={'class':'popup_registered'})\n",
    "            book = yes.string\n",
    "            link2 = yes['href']\n",
    "\n",
    "    except TimeoutException:\n",
    "        book = None\n",
    "        link2 = None\n",
    "\n",
    "    return book, link2\n",
    "\n",
    "#this function receives a movie title and year\n",
    "#it uses the 'MovieFirst' and 'BookSecond' functions from above\n",
    "#it returns the found movie title, the link to the movie entry, the book title, and the link to the book entry\n",
    "def ScrapeSearch(title, moviedate):\n",
    "\n",
    "        #url puzzle pieces\n",
    "        baseurl = 'https://www.librarything.com/search.php?search={}&searchtype=newwork_titles&searchtype=newwork_titles&sortchoice=0'\n",
    "        search = title + ' ['+ str(moviedate)+ ' film]'    \n",
    "   \n",
    "        #search by movie and date\n",
    "        newtitle, link1 = MovieFirst(baseurl, search)\n",
    "        #if no results, search by movie title only\n",
    "        if link1 == None:\n",
    "            newtitle, link1 = MovieFirst(baseurl, title)\n",
    "            #if still nothing, oh well\n",
    "            if link1 == None:\n",
    "                book = None\n",
    "                link2 =None\n",
    "            #otherwise, check that one for adapatation info\n",
    "            else:\n",
    "                book, link2 = BookSecond(link1)\n",
    "        #if movie results found, go into movie entry and get the book title and link\n",
    "        else:\n",
    "            book, link2 = BookSecond(link1)\n",
    "            #if no book adaptation info, search again by just the movie title\n",
    "            #assume first result without 'film]' is the book\n",
    "            #didn't filter by missing ] so this returned many bad results\n",
    "            if link2 == None:\n",
    "                book, link2 = MovieFirst(baseurl, title)\n",
    "            \n",
    "        #return them\n",
    "        return [newtitle, link1, book, link2]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe865fd5",
   "metadata": {},
   "source": [
    "Use functions defined above to create database of movies and corresponding source material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed6990e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   SearchTerm    99 non-null     object\n",
      " 1   Link1         99 non-null     object\n",
      " 2   AdaptationOf  89 non-null     object\n",
      " 3   Link2         89 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#pull in the IMDb list (1342 lines)\n",
    "moviesdb = pd.read_csv('CleanMovieData.csv')\n",
    "\n",
    "#initiate a dataframe to hold results\n",
    "ohgood = pd.DataFrame(columns = ['SearchTerm', 'Link1', 'AdaptationOf', 'Link2'])\n",
    "\n",
    "#batch maker\n",
    "moviesdb = moviesdb.iloc[1242:]\n",
    "\n",
    "#scrape\n",
    "for hm in moviesdb.index:\n",
    "    hello = ScrapeSearch(moviesdb.Title[hm], moviesdb.Date[hm])\n",
    "    #print(hello)\n",
    "    ohgood = ohgood.append(pd.Series(hello, index = ohgood.columns), ignore_index=True)\n",
    "\n",
    "print(ohgood.info())\n",
    "#ohgood.to_csv('LTlinks5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34139887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine batch files\n",
    "one = pd.read_csv('LTlinks1.csv')\n",
    "two = pd.read_csv('LTlinks2.csv')\n",
    "three = pd.read_csv('LTlinks3.csv')\n",
    "four = pd.read_csv('LTlinks4.csv')\n",
    "five = pd.read_csv('LTlinks5.csv')\n",
    "\n",
    "together = pd.concat([one,two,three,four,five], ignore_index=True)\n",
    "together.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "print(together.info())\n",
    "\n",
    "#together.to_csv('LTlinksFull.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea1bb3",
   "metadata": {},
   "source": [
    "# Second Step:\n",
    "Use list generated to scrape each novel page for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca94f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that scrapes an individual page\n",
    "def GetBookPlease(page):\n",
    "    \n",
    "    #print(page)\n",
    "    series = 0\n",
    "        \n",
    "    #get response\n",
    "    response = requests.get(url='https://www.librarything.com'+ page).text\n",
    "    soup = BeautifulSoup(response, features=\"html.parser\")\n",
    "    \n",
    "    #get top of page - author, publish date, if part of series\n",
    "    top = soup.find(attrs={'class':'headsummary'})\n",
    "    published = top.find(attrs={'class':'date'})\n",
    "    if published != None:\n",
    "        published = top.find(attrs={'class':'date'}).string\n",
    "    author = top.find('h2')\n",
    "    if author != None:\n",
    "        author = top.find('h2').text\n",
    "    stuff = top.find_all('h3')                          #this will always exist\n",
    "    for e in stuff:\n",
    "        if 'Series:' in e.text:\n",
    "            series = 1\n",
    "    \n",
    "    #get rating\n",
    "    rating = soup.find(attrs={'class':'dark_hint'})\n",
    "    if rating != None:\n",
    "        rating = soup.find(attrs={'class':'dark_hint'}).text\n",
    "        \n",
    "    #get number of characters\n",
    "    groups = soup.find_all(attrs={'class':'fwikiGroup'})\n",
    "    for g in range(len(groups)):\n",
    "        hm = groups[g].find(attrs={'fieldname':'characternames'})\n",
    "        if hm is not None:\n",
    "            place = g\n",
    "    charnum = groups[place].find(attrs={'class':'itemnumberoverflow'})\n",
    "    if charnum is None:\n",
    "        charnum = groups[place].find_all(attrs={'class':'fwikiAtomicValue'})\n",
    "        charnum = len(charnum)\n",
    "    else:\n",
    "        charnum = groups[place].find(attrs={'class':'itemnumberoverflow'}).text\n",
    "        charnum = re.findall(r'\\d+', charnum)[0]\n",
    "\n",
    "    #get number of awards\n",
    "    for g in range(len(groups)):\n",
    "        hm = groups[g].find(attrs={'fieldname':'awards'})\n",
    "        if hm is not None:\n",
    "            place = g\n",
    "    awardnum = groups[place].find(attrs={'class':'itemnumberoverflow'})\n",
    "    if awardnum is None:\n",
    "        awardnum = groups[place].find_all(attrs={'class':'fwikiAtomicValue'})\n",
    "        awardnum = len(awardnum)\n",
    "    else:\n",
    "        awardnum = groups[place].find(attrs={'class':'itemnumberoverflow'}).text\n",
    "        awardnum = re.findall(r'\\d+', awardnum)[0]\n",
    "        \n",
    "    #NOTE: this will still return values if the link is for a movie\n",
    "    return [page, published, author, series, rating, charnum, awardnum]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "116e8a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in the link list\n",
    "moviesdb = pd.read_csv('LTlinksFull.csv')\n",
    "\n",
    "#make a list of book links\n",
    "links = moviesdb['Link2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch maker\n",
    "links = links[576:].reset_index(drop=True)\n",
    "\n",
    "#initiate dataframe to hold returns\n",
    "library = pd.DataFrame(columns = ['link','published', 'author', 'series', 'rating', 'charnum', 'awardnum'])\n",
    "#links = ['/work/4041453', '/work/1906740', '/work/4725', '/work/5716']\n",
    "\n",
    "#run through all link2\n",
    "for l in range(len(links)):\n",
    "    if str(links[l]) != 'nan':\n",
    "        grab = GetBookPlease(links[l])\n",
    "        library = library.append(pd.Series(grab, index = library.columns), ignore_index=True)\n",
    "        \n",
    "#if there is no Link2 AND the last search term did not have ] in it, try Link1\n",
    "for m in range(len(links)):\n",
    "    if str(links[m])=='nan':\n",
    "        if r']' not in moviesdb.loc[m, 'SearchTerm']:\n",
    "            #print(moviesdb.loc[m, 'Link2'])\n",
    "            links[m] = moviesdb.loc[m, 'Link1']\n",
    "\n",
    "#print(library)\n",
    "print(library.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library.to_csv('LTScrape3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is just to slap everything together since I had trouble running the full scrape\n",
    "one = pd.read_csv('LTScrape1.csv')\n",
    "two = pd.read_csv('LTScrape2.csv')\n",
    "three = pd.read_csv('LTScrape3.csv')\n",
    "\n",
    "together = pd.concat([one,two,three])\n",
    "#together = together.applymap(str)\n",
    "together.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "print(together.info())\n",
    "\n",
    "together.to_csv('LTScrapeFull.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27b388d",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "Combine and clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the list and final sheets together\n",
    "one = pd.read_csv('LTlinksFull.csv')\n",
    "two = pd.read_csv('LTScrapeFull.csv')\n",
    "\n",
    "#merge matching Link2\n",
    "LT = one.merge(two, how='left', left_on = 'Link2', right_on = 'link')\n",
    "#print(LT.info())\n",
    "\n",
    "#only use rows where Link1 and Link2 are different\n",
    "#if they match, it was a return from where there was no adaptation info\n",
    "LT = LT[LT['Link1'] != LT['Link2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in any missing with Link1\n",
    "LT = LT.merge(two, how='left', left_on = 'Link1', right_on = 'link')\n",
    "\n",
    "LT['link_x'].fillna(LT['link_y'], inplace=True)\n",
    "LT['published_x'].fillna(LT['published_y'], inplace=True)\n",
    "LT['author_x'].fillna(LT['author_y'], inplace=True)\n",
    "LT['series_x'].fillna(LT['series_y'], inplace=True)\n",
    "LT['rating_x'].fillna(LT['rating_y'], inplace=True)\n",
    "LT['charnum_x'].fillna(LT['charnum_y'], inplace=True)\n",
    "LT['awardnum_x'].fillna(LT['awardnum_y'], inplace=True)\n",
    "\n",
    "LT.drop(['link_x','link_y','published_y','author_y','series_y','rating_y','charnum_y','awardnum_y'], axis=1, inplace=True)\n",
    "\n",
    "print(LT.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f881665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LT.to_csv('LibraryThingFull.csv',index=False)\n",
    "print(LT.info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
